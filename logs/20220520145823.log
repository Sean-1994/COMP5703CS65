ERROR - 2022-05-20 14:58:40,904 - process: 12544 - utils.py - polls.utils - 76 - utils - HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE86FD9840>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "E:\python\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "E:\python\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "E:\python\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "E:\python\lib\http\client.py", line 975, in send
    self.connect()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001EE86FD9840>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen
    retries = retries.increment(
  File "E:\python\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE86FD9840>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\5703\git_repository\polls\utils.py", line 74, in wrapper
    result = func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\generic\base.py", line 84, in view
    return self.dispatch(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "E:\python\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "E:\5703\git_repository\polls\views.py", line 168, in client_status
    requests.get(scrapyd_url(client.ip, client.port), timeout=3)
  File "E:\python\lib\site-packages\requests\api.py", line 75, in get
    return request('get', url, params=params, **kwargs)
  File "E:\python\lib\site-packages\requests\api.py", line 61, in request
    return session.request(method=method, url=url, **kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "E:\python\lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE86FD9840>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
ERROR - 2022-05-20 14:58:42,887 - process: 12544 - utils.py - polls.utils - 76 - utils - HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE870C2AD0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "E:\python\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "E:\python\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "E:\python\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "E:\python\lib\http\client.py", line 975, in send
    self.connect()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001EE870C2AD0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen
    retries = retries.increment(
  File "E:\python\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE870C2AD0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\5703\git_repository\polls\utils.py", line 74, in wrapper
    result = func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\generic\base.py", line 84, in view
    return self.dispatch(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "E:\python\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "E:\5703\git_repository\polls\views.py", line 342, in project_list
    projects = scrapyd.list_projects()
  File "E:\python\lib\site-packages\scrapyd_api\wrapper.py", line 148, in list_projects
    json = self.client.get(url, timeout=self.timeout)
  File "E:\python\lib\site-packages\requests\sessions.py", line 542, in get
    return self.request('GET', url, **kwargs)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 37, in request
    response = super(Client, self).request(*args, **kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "E:\python\lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE870C2AD0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
ERROR - 2022-05-20 14:58:45,444 - process: 12544 - utils.py - polls.utils - 76 - utils - HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE8717E260>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "E:\python\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "E:\python\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "E:\python\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "E:\python\lib\http\client.py", line 975, in send
    self.connect()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001EE8717E260>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen
    retries = retries.increment(
  File "E:\python\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE8717E260>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\5703\git_repository\polls\utils.py", line 74, in wrapper
    result = func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\generic\base.py", line 84, in view
    return self.dispatch(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "E:\python\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "E:\5703\git_repository\polls\views.py", line 342, in project_list
    projects = scrapyd.list_projects()
  File "E:\python\lib\site-packages\scrapyd_api\wrapper.py", line 148, in list_projects
    json = self.client.get(url, timeout=self.timeout)
  File "E:\python\lib\site-packages\requests\sessions.py", line 542, in get
    return self.request('GET', url, **kwargs)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 37, in request
    response = super(Client, self).request(*args, **kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "E:\python\lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE8717E260>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
ERROR - 2022-05-20 14:58:48,363 - process: 12544 - utils.py - polls.utils - 76 - utils - HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE87129120>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 95, in create_connection
    raise err
  File "E:\python\lib\site-packages\urllib3\util\connection.py", line 85, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "E:\python\lib\site-packages\urllib3\connection.py", line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File "E:\python\lib\http\client.py", line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "E:\python\lib\http\client.py", line 1328, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1277, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "E:\python\lib\http\client.py", line 1037, in _send_output
    self.send(msg)
  File "E:\python\lib\http\client.py", line 975, in send
    self.connect()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 205, in connect
    conn = self._new_conn()
  File "E:\python\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x000001EE87129120>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\python\lib\site-packages\requests\adapters.py", line 440, in send
    resp = conn.urlopen(
  File "E:\python\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen
    retries = retries.increment(
  File "E:\python\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE87129120>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "E:\5703\git_repository\polls\utils.py", line 74, in wrapper
    result = func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\generic\base.py", line 84, in view
    return self.dispatch(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "E:\python\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "E:\5703\git_repository\polls\views.py", line 342, in project_list
    projects = scrapyd.list_projects()
  File "E:\python\lib\site-packages\scrapyd_api\wrapper.py", line 148, in list_projects
    json = self.client.get(url, timeout=self.timeout)
  File "E:\python\lib\site-packages\requests\sessions.py", line 542, in get
    return self.request('GET', url, **kwargs)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 37, in request
    response = super(Client, self).request(*args, **kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 529, in request
    resp = self.send(prep, **send_kwargs)
  File "E:\python\lib\site-packages\requests\sessions.py", line 645, in send
    r = adapter.send(request, **kwargs)
  File "E:\python\lib\site-packages\requests\adapters.py", line 519, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPConnectionPool(host='127.0.0.1', port=6800): Max retries exceeded with url: /listprojects.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001EE87129120>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
ERROR - 2022-05-20 14:59:01,704 - process: 12544 - utils.py - polls.utils - 76 - utils - Traceback (most recent call last):
  File "E:\python\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "E:\python\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 46, in <module>
    main()
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 43, in main
    execute()
  File "E:\python\lib\site-packages\scrapy\cmdline.py", line 144, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 290, in __init__
    super().__init__(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 167, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 161, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 67, in from_settings
    return cls(settings)
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 24, in __init__
    self._load_all_spiders()
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 51, in _load_all_spiders
    for module in walk_modules(name):
  File "E:\python\lib\site-packages\scrapy\utils\misc.py", line 80, in walk_modules
    mod = import_module(path)
  File "E:\python\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "c:\users\shinelon\appdata\local\temp\health-1652861382-rnvg3vse.egg\health\spiders\__init__.py", line 2, in <module>
  File "E:\python\lib\site-packages\scrapy_redis\spiders.py", line 4, in <module>
    from collections import Iterable
ImportError: cannot import name 'Iterable' from 'collections' (E:\python\lib\collections\__init__.py)
Traceback (most recent call last):
  File "E:\5703\git_repository\polls\utils.py", line 74, in wrapper
    result = func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\generic\base.py", line 84, in view
    return self.dispatch(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "E:\python\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "E:\5703\git_repository\polls\views.py", line 233, in spider_list
    spiders = scrapyd.list_spiders(project_name)
  File "E:\python\lib\site-packages\scrapyd_api\wrapper.py", line 158, in list_spiders
    json = self.client.get(url, params=params, timeout=self.timeout)
  File "E:\python\lib\site-packages\requests\sessions.py", line 542, in get
    return self.request('GET', url, **kwargs)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 38, in request
    return self._handle_response(response)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 34, in _handle_response
    raise ScrapydResponseError(json['message'])
scrapyd_api.exceptions.ScrapydResponseError: Traceback (most recent call last):
  File "E:\python\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "E:\python\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 46, in <module>
    main()
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 43, in main
    execute()
  File "E:\python\lib\site-packages\scrapy\cmdline.py", line 144, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 290, in __init__
    super().__init__(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 167, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 161, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 67, in from_settings
    return cls(settings)
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 24, in __init__
    self._load_all_spiders()
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 51, in _load_all_spiders
    for module in walk_modules(name):
  File "E:\python\lib\site-packages\scrapy\utils\misc.py", line 80, in walk_modules
    mod = import_module(path)
  File "E:\python\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "c:\users\shinelon\appdata\local\temp\health-1652861382-rnvg3vse.egg\health\spiders\__init__.py", line 2, in <module>
  File "E:\python\lib\site-packages\scrapy_redis\spiders.py", line 4, in <module>
    from collections import Iterable
ImportError: cannot import name 'Iterable' from 'collections' (E:\python\lib\collections\__init__.py)

ERROR - 2022-05-20 14:59:04,523 - process: 12544 - utils.py - polls.utils - 76 - utils - Traceback (most recent call last):
  File "E:\python\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "E:\python\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 46, in <module>
    main()
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 43, in main
    execute()
  File "E:\python\lib\site-packages\scrapy\cmdline.py", line 144, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 290, in __init__
    super().__init__(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 167, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 161, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 67, in from_settings
    return cls(settings)
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 24, in __init__
    self._load_all_spiders()
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 51, in _load_all_spiders
    for module in walk_modules(name):
  File "E:\python\lib\site-packages\scrapy\utils\misc.py", line 80, in walk_modules
    mod = import_module(path)
  File "E:\python\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "c:\users\shinelon\appdata\local\temp\health-1652861382-3y11wln1.egg\health\spiders\__init__.py", line 2, in <module>
  File "E:\python\lib\site-packages\scrapy_redis\spiders.py", line 4, in <module>
    from collections import Iterable
ImportError: cannot import name 'Iterable' from 'collections' (E:\python\lib\collections\__init__.py)
Traceback (most recent call last):
  File "E:\5703\git_repository\polls\utils.py", line 74, in wrapper
    result = func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\decorators\csrf.py", line 54, in wrapped_view
    return view_func(*args, **kwargs)
  File "E:\python\lib\site-packages\django\views\generic\base.py", line 84, in view
    return self.dispatch(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 509, in dispatch
    response = self.handle_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 469, in handle_exception
    self.raise_uncaught_exception(exc)
  File "E:\python\lib\site-packages\rest_framework\views.py", line 480, in raise_uncaught_exception
    raise exc
  File "E:\python\lib\site-packages\rest_framework\views.py", line 506, in dispatch
    response = handler(request, *args, **kwargs)
  File "E:\python\lib\site-packages\rest_framework\decorators.py", line 50, in handler
    return func(*args, **kwargs)
  File "E:\5703\git_repository\polls\views.py", line 233, in spider_list
    spiders = scrapyd.list_spiders(project_name)
  File "E:\python\lib\site-packages\scrapyd_api\wrapper.py", line 158, in list_spiders
    json = self.client.get(url, params=params, timeout=self.timeout)
  File "E:\python\lib\site-packages\requests\sessions.py", line 542, in get
    return self.request('GET', url, **kwargs)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 38, in request
    return self._handle_response(response)
  File "E:\python\lib\site-packages\scrapyd_api\client.py", line 34, in _handle_response
    raise ScrapydResponseError(json['message'])
scrapyd_api.exceptions.ScrapydResponseError: Traceback (most recent call last):
  File "E:\python\lib\runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "E:\python\lib\runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 46, in <module>
    main()
  File "E:\python\lib\site-packages\scrapyd\runner.py", line 43, in main
    execute()
  File "E:\python\lib\site-packages\scrapy\cmdline.py", line 144, in execute
    cmd.crawler_process = CrawlerProcess(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 290, in __init__
    super().__init__(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 167, in __init__
    self.spider_loader = self._get_spider_loader(settings)
  File "E:\python\lib\site-packages\scrapy\crawler.py", line 161, in _get_spider_loader
    return loader_cls.from_settings(settings.frozencopy())
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 67, in from_settings
    return cls(settings)
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 24, in __init__
    self._load_all_spiders()
  File "E:\python\lib\site-packages\scrapy\spiderloader.py", line 51, in _load_all_spiders
    for module in walk_modules(name):
  File "E:\python\lib\site-packages\scrapy\utils\misc.py", line 80, in walk_modules
    mod = import_module(path)
  File "E:\python\lib\importlib\__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "c:\users\shinelon\appdata\local\temp\health-1652861382-3y11wln1.egg\health\spiders\__init__.py", line 2, in <module>
  File "E:\python\lib\site-packages\scrapy_redis\spiders.py", line 4, in <module>
    from collections import Iterable
ImportError: cannot import name 'Iterable' from 'collections' (E:\python\lib\collections\__init__.py)

